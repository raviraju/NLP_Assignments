Assignment 3 Report

Name: Ravi Raju Krishna

1. If you included files other than baseline_crf.py, advanced_crf.py, evaluate_model.py, and hw3_corpus_tool.py, 
   or modified hw3_corpus_tool.py please describe what the files do and/or your modifications to hw3_corpus_tool.py.
   
   hw3_corpus_tool.py : was updated with a new function get_data_fileName(data_dir), 
			which shall return a dictionary of dialog filename and data of lists of utterances from each dialog file
   split_dataset_train_dev.py:
			partition dataset into train and dev picking files in random from directory passed as an arg , with percentage specified
    $python split_dataset_train_dev.py labeled\ data 75 25
    #total_no_of_files : 1076
    #train_no_of_files : 807
    #dev_no_of_files : 269
    #Copying files into train and dev folders...
    #TrainingData Dir : labeled data_807 No Of Files : 807
    #DevelopmentData Dir : labeled data_269 No Of Files : 269
=======================================================================================================================================================================================================================================================================================
2. Describe how you evaluated your baseline and advanced features
    a. leveraging split_dataset_train_dev.py labelled data was split by:
       75%-25% (labeled data_807/, labeled data_269/)
       and 
       93%-7%, (labeled data_1000/, labeled data_76/)
       so as to get 2 groups of datasets for train and development
    b. Feature vector for each utterance, was created for 
       baseline_crf.py(as per assignment guidelines) and 
       advanced_crf.py(includes 3 new features along with features for baseline)
	
	Sample Data(0001.csv):
	1    act_tag	speaker	pos	text
	2    qw	A	What/WP are/VBP your/PRP$ favorite/JJ programs/NNS ?/.	What are your favorite programs? /
	3    sd	B	Uh/UH ,/, it/PRP 's/BES kind/RB of/RB hard/JJ to/TO put/VB my/PRP$ finger/NN on/IN a/DT ,/, on/IN a/DT favorite/JJ T/NN V/NN program/NN ,/,	{F Uh, } it's kind of hard to put my finger [ on a, + on a ] favorite T V program, /
	4    sd	B	however/RB ,/, uh/UH ,/, one/CD that/WDT I/PRP 've/VBP been/VBN watching/VBG for/IN a/DT number/NN of/IN years/NNS is/VBZ DALLAS/NNP ./.	{C however, } {F uh, } one that I've been watching for a number of years is DALLAS. /
	5    sd	B	And/CC ,/, uh/UH ,/,	[ {C And, } + {F uh, }
	6    ba	A	Oh/UH ,/, how/WRB funny/JJ ./.	{F Oh, } how funny. /
	    
	63   sv	A	I/PRP think/VBP that/DT is/VBZ just/RB a/DT hoot/NN ./.	I think that is just a hoot. /

	    
	Baseline_Features
	    (FirstUtterance(1/0), SpeakerChange(1/0), Word tokens of utterance with prefix 'TOKEN_', Part of Speech tags of utterance with prefix 'POS_')
	    Is Convention followed to generate the following features:
	    
	    ['1', '0', 'TOKEN_What', 'TOKEN_are', 'TOKEN_your', 'TOKEN_favorite', 'TOKEN_programs', 'TOKEN_?', 'POS_WP', 'POS_VBP', 'POS_PRP$', 'POS_JJ', 'POS_NNS', 'POS_.']
	    ['0', '1', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_it', "TOKEN_'s", 'TOKEN_kind', 'TOKEN_of', 'TOKEN_hard', 'TOKEN_to', 'TOKEN_put', 'TOKEN_my', 'TOKEN_finger', 'TOKEN_on', 'TOKEN_a', 'TOKEN_,', 'TOKEN_on', 'TOKEN_a', 'TOKEN_favorite', 'TOKEN_T', 'TOKEN_V', 'TOKEN_program', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_BES', 'POS_RB', 'POS_RB', 'POS_JJ', 'POS_TO', 'POS_VB', 'POS_PRP$', 'POS_NN', 'POS_IN', 'POS_DT', 'POS_,', 'POS_IN', 'POS_DT', 'POS_JJ', 'POS_NN', 'POS_NN', 'POS_NN', 'POS_,']
	    ['0', '0', 'TOKEN_however', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_one', 'TOKEN_that', 'TOKEN_I', "TOKEN_'ve", 'TOKEN_been', 'TOKEN_watching', 'TOKEN_for', 'TOKEN_a', 'TOKEN_number', 'TOKEN_of', 'TOKEN_years', 'TOKEN_is', 'TOKEN_DALLAS', 'TOKEN_.', 'POS_RB', 'POS_,', 'POS_UH', 'POS_,', 'POS_CD', 'POS_WDT', 'POS_PRP', 'POS_VBP', 'POS_VBN', 'POS_VBG', 'POS_IN', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NNS', 'POS_VBZ', 'POS_NNP', 'POS_.']
	    ['0', '0', 'TOKEN_And', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'POS_CC', 'POS_,', 'POS_UH', 'POS_,']
	    ['0', '1', 'TOKEN_Oh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_funny', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_JJ', 'POS_.']
	
	    ['0', '1', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'TOKEN_is', 'TOKEN_just', 'TOKEN_a', 'TOKEN_hoot', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_DT', 'POS_VBZ', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_.']
	    
	Advanced_Features
	    ((Question/Response)if_applicable, FirstUtterance(1/0), SpeakerChange(1/0),(PREV_TO_PREV_act_tag, PREV_act_tag) for all utterances except first 2, (indicator for sv_tag)if applicable, Word tokens of utterance with prefix 'TOKEN_', Part of Speech tags of utterance with prefix 'POS_')
	    
	    ['Question', '1', '0', 'TOKEN_What', 'TOKEN_are', 'TOKEN_your', 'TOKEN_favorite', 'TOKEN_programs', 'TOKEN_?', 'POS_WP', 'POS_VBP', 'POS_PRP$', 'POS_JJ', 'POS_NNS', 'POS_.']
	    ['Response', '0', '1', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_it', "TOKEN_'s", 'TOKEN_kind', 'TOKEN_of', 'TOKEN_hard', 'TOKEN_to', 'TOKEN_put', 'TOKEN_my', 'TOKEN_finger', 'TOKEN_on', 'TOKEN_a', 'TOKEN_,', 'TOKEN_on', 'TOKEN_a', 'TOKEN_favorite', 'TOKEN_T', 'TOKEN_V', 'TOKEN_program', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_BES', 'POS_RB', 'POS_RB', 'POS_JJ', 'POS_TO', 'POS_VB', 'POS_PRP$', 'POS_NN', 'POS_IN', 'POS_DT', 'POS_,', 'POS_IN', 'POS_DT', 'POS_JJ', 'POS_NN', 'POS_NN', 'POS_NN', 'POS_,']
	    ['0', '0', 'PREV_TO_PREV_qw', 'PREV_sd', 'TOKEN_however', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_one', 'TOKEN_that', 'TOKEN_I', "TOKEN_'ve", 'TOKEN_been', 'TOKEN_watching', 'TOKEN_for', 'TOKEN_a', 'TOKEN_number', 'TOKEN_of', 'TOKEN_years', 'TOKEN_is', 'TOKEN_DALLAS', 'TOKEN_.', 'POS_RB', 'POS_,', 'POS_UH', 'POS_,', 'POS_CD', 'POS_WDT', 'POS_PRP', 'POS_VBP', 'POS_VBN', 'POS_VBG', 'POS_IN', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NNS', 'POS_VBZ', 'POS_NNP', 'POS_.']
	    ['0', '0', 'PREV_TO_PREV_sd', 'PREV_sd', 'TOKEN_And', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'POS_CC', 'POS_,', 'POS_UH', 'POS_,']
	    ['0', '1', 'PREV_TO_PREV_sd', 'PREV_sd', 'TOKEN_Oh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_funny', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_JJ', 'POS_.']
	    
	    ['0', '1', 'PREV_TO_PREV_sv', 'PREV_sd', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'TOKEN_is', 'TOKEN_just', 'TOKEN_a', 'TOKEN_hoot', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_DT', 'POS_VBZ', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_.']
    c. CRF model was built using default L-BFGS training algorithm with Elastic Net (L1 + L2) regularization,as conveyed in reference : http://nbviewer.jupyter.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb
       Much assignment focus was spent on designing advanced NLP features which could aid in sequence labelling, keeping the rest of hyperparameters of algorithm intact
       No of training iterations was varied to get both baseline and advanced crf models to be trained within 600 sec.
=======================================================================================================================================================================================================================================================================================
3. Describe your advanced feature set.
    As described in 4.d, we have +8.896220522% accuracy gain compared to baseline on 75%-25% data split using combination of following 3 features in addition to baseline features:
    1) sv_True : To aid CRF distinguish between sd and sv act_tags, this clue feature is enabled on act_tag which have "sv" as their label
                 and have ('PRP', 'MD'), ('PRP', 'VBP'), ('PRP', 'VBD') sequence of POS_tag pairs to capture opinion chunk text such as 
		 you/PRP should/MD, you/PRP can/MD, I/PRP think/VBP, I/PRP thought/VBD
    2) Question/Response : 
		Feature to indicate what utterance constitutes a question(based on question act_tags) and 
		response would be followed by a speaker change
		questionTagSet = {'qy','qw','qy^d','bh','qo', 'qh', '^g', 'qw^d'}

    3) PREV_TO_PREV_*, PREV_* :
		To provide context of labels(act_tags)of previous 2 utterances
    Sample for combined features of uttereances
	qh	['Question', '0', '0', 'PREV_TO_PREV_sv', 'PREV_h', 'TOKEN_how', 'TOKEN_many', 'TOKEN_years', 'TOKEN_have', 'TOKEN_we', 'TOKEN_tried', 'TOKEN_to', 'TOKEN_find', 'TOKEN_a', 'TOKEN_cure', 'TOKEN_for', 'TOKEN_cancer', 'TOKEN_.', 'POS_WRB', 'POS_JJ', 'POS_NNS', 'POS_VBP', 'POS_PRP', 'POS_VBN', 'POS_TO', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_.']
	b	['Response', '0', '1', 'PREV_TO_PREV_h', 'PREV_qh', 'TOKEN_Uh-huh', 'TOKEN_.', 'POS_UH', 'POS_.']
	sv	['0', '1', 'PREV_TO_PREV_qh', 'PREV_b', 'sv_True', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_it', "TOKEN_'ll", 'TOKEN_probably', 'TOKEN_be', 'TOKEN_as', 'TOKEN_difficult', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_PRP', 'POS_MD', 'POS_RB', 'POS_VB', 'POS_RB', 'POS_JJ', 'POS_.']
	qo	['Question', '0', '0', 'PREV_TO_PREV_b', 'PREV_sv', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_about', 'TOKEN_yourself', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_IN', 'POS_PRP', 'POS_.']
	sd	['Response', '0', '1', 'PREV_TO_PREV_sv', 'PREV_qo', 'TOKEN_Um', 'TOKEN_,', 'TOKEN_well', 'TOKEN_,', 'TOKEN_I', "TOKEN_'m", 'TOKEN_certainly', 'TOKEN_in', 'TOKEN_favor', 'TOKEN_of', 'TOKEN_AIDS', 'TOKEN_research', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_IN', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_NN', 'POS_,']
	sv	['0', '0', 'PREV_TO_PREV_qo', 'PREV_sd', 'sv_True', 'TOKEN_and', 'TOKEN_I', 'TOKEN_think', 'TOKEN_probably', 'TOKEN_the', 'TOKEN_government', 'TOKEN_could', 'TOKEN_spend', 'TOKEN_a', 'TOKEN_bit', 'TOKEN_more', 'TOKEN_money', 'TOKEN_on', 'TOKEN_it', 'TOKEN_than', 'TOKEN_it', 'TOKEN_does', 'TOKEN_.', 'POS_CC', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_MD', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_JJR', 'POS_NN', 'POS_IN', 'POS_PRP', 'POS_IN', 'POS_PRP', 'POS_VBZ', 'POS_.']
	sv	['0', '0', 'PREV_TO_PREV_sd', 'PREV_sv', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'POS_PRP', 'POS_VBP', 'POS_DT']

	Overall Accuracy : 82.09237857254412	allCombo_100_50_advanced_result.txt, allCombo_100_50_resultAnalyze.txt
	Accuracy Gain (compared to baseline features): 82.09237857254412 - 73.19615805091362 = +8.896220522%
=======================================================================================================================================================================================================================================================================================
4. If you tried alternate advanced feature sets, please describe them.
   a. From baseline features crf model it was observed the majority (3440) of sd(Statement-non-opinion) and sv(Statement-opinion) tags where getting missclassified for eachother
	 ['1', '0', 'TOKEN_What', 'TOKEN_are', 'TOKEN_your', 'TOKEN_favorite', 'TOKEN_programs', 'TOKEN_?', 'POS_WP', 'POS_VBP', 'POS_PRP$', 'POS_JJ', 'POS_NNS', 'POS_.']
	 ('_dssv', 3440),
	 ('_aab', 1377),
	 ('+_ds', 1358),
	 ('%_ds', 895),
	 ('+_sv', 472),
	 ('%_b', 375),
	 Overall Accuracy(baseline features) : 73.19615805091362	plain_100_50_advanced_result.txt, plain_100_50_resultAnalyze.txt
	 So a feature to present a heuristic hint about sv(Statement-opinion) class was analysed:
	 The following are some of sv tagged utterances :
	    Well/UH then/RB ,/, you/PRP should/MD be/VB a/DT good/JJ one/CD to/TO know/VB because/IN I/PRP ,/, my/PRP$ children/NNS are/VBP grown/JJ now/RB ./.
	    you/PRP should/MD
	    Well/UH you/PRP can/MD always/RB get/VB up/RB and/CC leave/VB that/DT ./.
	    you/PRP can/MD
	    and/CC besides/IN ,/, I/PRP can/MD be/VB doing/VBG other/JJ things/NNS and/CC still/RB listen/VB to/IN the/DT news/NN ./.
	    I/PRP can/MD
	    And/CC you/PRP could/MD always/RB find/VB some/DT channel/NN that/WDT had/VBD something/NN on/RP
	    And/CC uh/UH ,/, you/PRP could/MD always/RB catch/VB a/DT good/JJ news/NN program/NN at/IN eight/CD o'clock/RB or/CC nine/CD o'clock/RB on/IN C/NNP N/NNP N/NNP ./.
	    and/CC then/RB you/PRP could/MD go/VB on/RP ,/, you/PRP know/VBP ,/, to/TO do/VB what/WP else/RB you/PRP wanted/VBD to/TO do/VB ./.
	    you/PRP could/MD

	    Well/UH ,/, I/PRP think/VBP the/DT public/JJ school/NN systems/NNS are/VBP doing/VBG a/DT good/JJ job/NN ./.
	    I/PRP think/VBP
	    I/PRP ,/, I/PRP agree/VBP with/IN you/PRP that/IN they/PRP 're/VBP starting/VBG children/NNS so/RB much/RB earlier/RBR on/IN things/NNS because/IN our/PRP$ grandchildren/NNS ,/,
	    I/PRP agree/VBP

	    and/CC then/RB ,/, you/PRP know/VBP ,/, the/DT special/JJ reports/NNS and/CC the/DT extended/JJ news/NN coverage/NN I/PRP thought/VBD was/VBD really/RB good/JJ ./.
	    I/PRP thought/VBD
	    but/CC you/PRP did/VBD n't/RB necessarily/RB have/VB to/TO watch/VB the/DT same/JJ thing/NN all/PDT the/DT time/NN ./.
	    you/PRP did/VBD n't/RB

	    and/CC MUMBLEx/XX ./. You/PRP know/VBP ,/, you/PRP can/MD as/RB far/RB as/IN I/PRP 'm/VBP concerned/JJ you/PRP can/MD make/VB a/DT survey/NN or/CC test/NN scores/NNS say/VB anything/NN you/PRP want/VBP it/PRP to/TO say/VB ./.
	    I/PRP 'm/VBP concerned/JJ
	    but/CC those/DT are/VBP ni/JJ ,/, Germantown/NNP is/VBZ a/DT nice/JJ area/NN ./.
	    It/PRP 's/BES a/DT nice/JJ area/NN ./.
	    nice/JJ area/NN
	posPairSet = {('PRP', 'MD'), ('PRP', 'VBP'), ('PRP', 'VBD')}
	['0', '1', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'TOKEN_is', 'TOKEN_just', 'TOKEN_a', 'TOKEN_hoot', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_DT', 'POS_VBZ', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_.']
	we have one sv_True feature added for the occurence : 'TOKEN_I', 'TOKEN_think'	'POS_PRP', 'POS_VBP'
	['0', '0', 'sv_True', 'sv_True', 'TOKEN_So', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_they', 'TOKEN_need', 'TOKEN_to', 'TOKEN_look', 'TOKEN_into', 'TOKEN_it', 'POS_RB', 'POS_PRP', 'POS_VBP', 'POS_IN', 'POS_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_VBP', 'POS_TO', 'POS_VB', 'POS_IN', 'POS_PRP']
	('+_ds', 1424),
	('_aab', 1353),
	('_dssv', 1120),
	('%_ds', 904),
	('%_b', 388),
	('_bny', 263),
	Overall Accuracy : 78.44955489614243	sv_100_50_advanced_result.txt, sv_100_50_resultAnalyse.txt
	
	Hence with sv_True feature, we were able to reduce no of misclassification errors from ('_dssv', 3440), to ('_dssv', 1120), 
	by order of magnitude(x3)
	Accuracy Gain (compared to baseline features): 78.44955489614243% - 73.19615805091362% = +5.253396845%
    
    b. Some heuristic to indicate what utterance constitutes a question(based on question act_tags) and 
	response would be followed by a speaker change
	questionTagSet = {'qy','qw','qy^d','bh','qo', 'qh', '^g', 'qw^d'}
	on speaker change if previous one is a question current should be a response

	qy	['Question', '0', '1', 'TOKEN_Is', 'TOKEN_that', 'TOKEN_,', 'POS_VBZ', 'POS_DT', 'POS_,']
	qy	['Question', '0', '0', 'TOKEN_that', 'TOKEN_is', 'TOKEN_right', 'TOKEN_,', 'TOKEN_is', "TOKEN_n't", 'TOKEN_it', 'TOKEN_?', 'POS_DT', 'POS_VBZ', 'POS_JJ', 'POS_,', 'POS_VBZ', 'POS_RB', 'POS_PRP', 'POS_.']
	ny	['Response', '0', '1', 'TOKEN_Yeah', 'TOKEN_.', 'POS_UH', 'POS_.']
	sd	['0', '1', 'TOKEN_Because', 'TOKEN_they', 'TOKEN_ask', 'TOKEN_me', 'TOKEN_,', 'TOKEN_why', 'TOKEN_are', 'TOKEN_there', 'TOKEN_two', 'TOKEN_bridges', 'TOKEN_going', 'TOKEN_into', 'TOKEN_Dallas', 'TOKEN_?', 'POS_IN', 'POS_PRP', 'POS_VBP', 'POS_PRP', 'POS_,', 'POS_WRB', 'POS_VBP', 'POS_EX', 'POS_CD', 'POS_NNS', 'POS_VBG', 'POS_IN', 'POS_NNP', 'POS_.']

	Overall Accuracy : 74.60370138997345	questionAns_100_50_advanced_result.txt, questionAns_100_50_resultAnalyze.txt
	Accuracy Gain (compared to baseline features): 74.60370138997345% - 73.19615805091362% = +1.407543339%
    
    c. Context info with act_tag labels of previous 2 utterances(expect first 2 of a conversation)
	qw	['1', '0', 'TOKEN_What', 'TOKEN_are', 'TOKEN_your', 'TOKEN_favorite', 'TOKEN_programs', 'TOKEN_?', 'POS_WP', 'POS_VBP', 'POS_PRP$', 'POS_JJ', 'POS_NNS', 'POS_.']
	sd	['0', '1', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_it', "TOKEN_'s", 'TOKEN_kind', 'TOKEN_of', 'TOKEN_hard', 'TOKEN_to', 'TOKEN_put', 'TOKEN_my', 'TOKEN_finger', 'TOKEN_on', 'TOKEN_a', 'TOKEN_,', 'TOKEN_on', 'TOKEN_a', 'TOKEN_favorite', 'TOKEN_T', 'TOKEN_V', 'TOKEN_program', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_BES', 'POS_RB', 'POS_RB', 'POS_JJ', 'POS_TO', 'POS_VB', 'POS_PRP$', 'POS_NN', 'POS_IN', 'POS_DT', 'POS_,', 'POS_IN', 'POS_DT', 'POS_JJ', 'POS_NN', 'POS_NN', 'POS_NN', 'POS_,']
	sd	['0', '0', 'PREV_TO_PREV_qw', 'PREV_sd', 'TOKEN_however', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_one', 'TOKEN_that', 'TOKEN_I', "TOKEN_'ve", 'TOKEN_been', 'TOKEN_watching', 'TOKEN_for', 'TOKEN_a', 'TOKEN_number', 'TOKEN_of', 'TOKEN_years', 'TOKEN_is', 'TOKEN_DALLAS', 'TOKEN_.', 'POS_RB', 'POS_,', 'POS_UH', 'POS_,', 'POS_CD', 'POS_WDT', 'POS_PRP', 'POS_VBP', 'POS_VBN', 'POS_VBG', 'POS_IN', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NNS', 'POS_VBZ', 'POS_NNP', 'POS_.']
	sd	['0', '0', 'PREV_TO_PREV_sd', 'PREV_sd', 'TOKEN_And', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'POS_CC', 'POS_,', 'POS_UH', 'POS_,']
	ba	['0', '1', 'PREV_TO_PREV_sd', 'PREV_sd', 'TOKEN_Oh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_funny', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_JJ', 'POS_.']
	+	['0', '1', 'PREV_TO_PREV_sd', 'PREV_ba', 'TOKEN_And', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_it', "TOKEN_'s", 'TOKEN_going', 'TOKEN_to', 'TOKEN_be', 'TOKEN_going', 'TOKEN_off', 'TOKEN_the', 'TOKEN_air', 'TOKEN_,', 'TOKEN_uh', 'TOKEN_,', 'TOKEN_let', "TOKEN_'s", 'TOKEN_see', 'TOKEN_,', 'TOKEN_a', 'TOKEN_week', 'TOKEN_from', 'TOKEN_Fri-', 'TOKEN_,', 'TOKEN_a', 'TOKEN_week', 'TOKEN_from', 'TOKEN_tonight', 'TOKEN_.', 'POS_CC', 'POS_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_BES', 'POS_VBG', 'POS_TO', 'POS_VB', 'POS_VBG', 'POS_IN', 'POS_DT', 'POS_NN', 'POS_,', 'POS_UH', 'POS_,', 'POS_VB', 'POS_PRP', 'POS_VB', 'POS_,', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NNP', 'POS_,', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_RB', 'POS_.']

	Overall Accuracy : 77.18842729970326	prevContext_100_50_advanced_result.txt, prevContext_100_50_resultAnalyze.txt
	Accuracy Gain (compared to baseline features): 77.18842729970326% - 73.19615805091362% = +3.992269249%
    
    d. Combination of 4.a,4.b,4.c features
    
	qh	['Question', '0', '0', 'PREV_TO_PREV_sv', 'PREV_h', 'TOKEN_how', 'TOKEN_many', 'TOKEN_years', 'TOKEN_have', 'TOKEN_we', 'TOKEN_tried', 'TOKEN_to', 'TOKEN_find', 'TOKEN_a', 'TOKEN_cure', 'TOKEN_for', 'TOKEN_cancer', 'TOKEN_.', 'POS_WRB', 'POS_JJ', 'POS_NNS', 'POS_VBP', 'POS_PRP', 'POS_VBN', 'POS_TO', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_.']
	b	['Response', '0', '1', 'PREV_TO_PREV_h', 'PREV_qh', 'TOKEN_Uh-huh', 'TOKEN_.', 'POS_UH', 'POS_.']
	sv	['0', '1', 'PREV_TO_PREV_qh', 'PREV_b', 'sv_True', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_it', "TOKEN_'ll", 'TOKEN_probably', 'TOKEN_be', 'TOKEN_as', 'TOKEN_difficult', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_PRP', 'POS_MD', 'POS_RB', 'POS_VB', 'POS_RB', 'POS_JJ', 'POS_.']
	qo	['Question', '0', '0', 'PREV_TO_PREV_b', 'PREV_sv', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_about', 'TOKEN_yourself', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_IN', 'POS_PRP', 'POS_.']
	sd	['Response', '0', '1', 'PREV_TO_PREV_sv', 'PREV_qo', 'TOKEN_Um', 'TOKEN_,', 'TOKEN_well', 'TOKEN_,', 'TOKEN_I', "TOKEN_'m", 'TOKEN_certainly', 'TOKEN_in', 'TOKEN_favor', 'TOKEN_of', 'TOKEN_AIDS', 'TOKEN_research', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_IN', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_NN', 'POS_,']
	sv	['0', '0', 'PREV_TO_PREV_qo', 'PREV_sd', 'sv_True', 'TOKEN_and', 'TOKEN_I', 'TOKEN_think', 'TOKEN_probably', 'TOKEN_the', 'TOKEN_government', 'TOKEN_could', 'TOKEN_spend', 'TOKEN_a', 'TOKEN_bit', 'TOKEN_more', 'TOKEN_money', 'TOKEN_on', 'TOKEN_it', 'TOKEN_than', 'TOKEN_it', 'TOKEN_does', 'TOKEN_.', 'POS_CC', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_MD', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_JJR', 'POS_NN', 'POS_IN', 'POS_PRP', 'POS_IN', 'POS_PRP', 'POS_VBZ', 'POS_.']
	sv	['0', '0', 'PREV_TO_PREV_sd', 'PREV_sv', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'POS_PRP', 'POS_VBP', 'POS_DT']

	Overall Accuracy : 82.09237857254412	allCombo_100_50_advanced_result.txt, allCombo_100_50_resultAnalyze.txt
	Accuracy Gain (compared to baseline features): 82.09237857254412 - 73.19615805091362 = +8.896220522%
	
    e. After analysis of 4.d, it was later found tat just using a single PREV_act_tag had a better overall performance when compared to 2 previous act_tags
	
	qh	['Question', '0', '0', 'PREV_h', 'TOKEN_how', 'TOKEN_many', 'TOKEN_years', 'TOKEN_have', 'TOKEN_we', 'TOKEN_tried', 'TOKEN_to', 'TOKEN_find', 'TOKEN_a', 'TOKEN_cure', 'TOKEN_for', 'TOKEN_cancer', 'TOKEN_.', 'POS_WRB', 'POS_JJ', 'POS_NNS', 'POS_VBP', 'POS_PRP', 'POS_VBN', 'POS_TO', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_.']
	b	['Response', '0', '1', 'PREV_qh', 'TOKEN_Uh-huh', 'TOKEN_.', 'POS_UH', 'POS_.']
	sv	['0', '1', 'PREV_b', 'sv_True', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_it', "TOKEN_'ll", 'TOKEN_probably', 'TOKEN_be', 'TOKEN_as', 'TOKEN_difficult', 'TOKEN_.', 'POS_PRP', 'POS_VBP', 'POS_PRP', 'POS_MD', 'POS_RB', 'POS_VB', 'POS_RB', 'POS_JJ', 'POS_.']
	qo	['Question', '0', '0', 'PREV_sv', 'TOKEN_Uh', 'TOKEN_,', 'TOKEN_how', 'TOKEN_about', 'TOKEN_yourself', 'TOKEN_.', 'POS_UH', 'POS_,', 'POS_WRB', 'POS_IN', 'POS_PRP', 'POS_.']
	sd	['Response', '0', '1', 'PREV_qo', 'TOKEN_Um', 'TOKEN_,', 'TOKEN_well', 'TOKEN_,', 'TOKEN_I', "TOKEN_'m", 'TOKEN_certainly', 'TOKEN_in', 'TOKEN_favor', 'TOKEN_of', 'TOKEN_AIDS', 'TOKEN_research', 'TOKEN_,', 'POS_UH', 'POS_,', 'POS_UH', 'POS_,', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_IN', 'POS_NN', 'POS_IN', 'POS_NN', 'POS_NN', 'POS_,']
	sv	['0', '0', 'PREV_sd', 'sv_True', 'TOKEN_and', 'TOKEN_I', 'TOKEN_think', 'TOKEN_probably', 'TOKEN_the', 'TOKEN_government', 'TOKEN_could', 'TOKEN_spend', 'TOKEN_a', 'TOKEN_bit', 'TOKEN_more', 'TOKEN_money', 'TOKEN_on', 'TOKEN_it', 'TOKEN_than', 'TOKEN_it', 'TOKEN_does', 'TOKEN_.', 'POS_CC', 'POS_PRP', 'POS_VBP', 'POS_RB', 'POS_DT', 'POS_NN', 'POS_MD', 'POS_VB', 'POS_DT', 'POS_NN', 'POS_JJR', 'POS_NN', 'POS_IN', 'POS_PRP', 'POS_IN', 'POS_PRP', 'POS_VBZ', 'POS_.']
	sv	['0', '0', 'PREV_sv', 'sv_True', 'TOKEN_I', 'TOKEN_think', 'TOKEN_that', 'POS_PRP', 'POS_VBP', 'POS_DT']
	Overall Accuracy : 82.09433078244572
	Accuracy Gain (compared to baseline features): 82.09433078244572 - 73.19615805091362 = +8.898172732%
=======================================================================================================================================================================================================================================================================================
Summary : 
Iterations		100			50
Accuracy		Baseline		Advanced_2_PREV_act_tags	Advanced_1_PREV_act_tag
Local_data_75%_25%	73.52412931438388	82.09237857254412		82.09433078244572
Local_data_93%_7%	74.00907513679435	82.04324035766716
Vocareum		72.2622478386		81.2418129421
=======================================================================================================================================================================================================================================================================================
5. Accuracy of baseline features was:

    Local DataSet 75%-25%:
	python34 baseline_crf.py ../../labeled\ data_807/ ../../labeled\ data_269/ 75_25__100_50_baseline_result.txt
	    Loaded Training Data
	    Trained Baseline_CRF model with 100 iterations
	    Predicted Sequential Labels for Test Data
	    --- 163.23847579956055 seconds ---
	python34 evaluate_model.py ../../labeled\ data_269/ 75_25__100_50_baseline_result.txt
	    Overall Accuracy : 73.52412931438388
	mv resultAnalyze.txt 75_25__100_50_resultAnalyze_baseline.txt

    Local DataSet 93%-7%:
	python34 baseline_crf.py ../../labeled\ data_1000/ ../../labeled\ data_76/ 93_7__100_50_baseline_result.txt
	    Loaded Training Data
	    Trained Baseline_CRF model with 100 iterations
	    Predicted Sequential Labels for Test Data
	    --- 195.27298092842102 seconds ---
	python34 evaluate_model.py ../../labeled\ data_76/ 93_7__100_50_baseline_result.txt
	    Overall Accuracy : 74.00907513679435
	mv resultAnalyze.txt 93_7__100_50_resultAnalyze_baseline.txt

    Vocareum:
	Trained Baseline_CRF model with 100 iterations 
	Predicted Sequential Labels for Test Data 
	--- 233.19680738449097 seconds --- 
	Number of correct labels: 11033/15268
	Accuracy: 72.2622478386%
=======================================================================================================================================================================================================================================================================================  
6. Accuracy of advanced features was:

    Local DataSet 75%-25%:
	python34 advanced_crf.py ../../labeled\ data_807/ ../../labeled\ data_269/ 75_25__100_50_advanced_result.txt
	    Loaded Training Data
	    Trained Advanced_CRF model with 50 iterations
	    Predicted Sequential Labels for Test Data
	    --- 91.10075616836548 seconds ---
	python34 evaluate_model.py ../../labeled\ data_269/ 75_25__100_50_advanced_result.txt
	    Overall Accuracy : 82.09237857254412
	mv resultAnalyze.txt 75_25__100_50_resultAnalyze_advanced.txt

    Local DataSet 93%-7%:
	python34 advanced_crf.py ../../labeled\ data_1000/ ../../labeled\ data_76/ 93_7__100_50_advanced_result.txt
	    Loaded Training Data
	    Trained Advanced_CRF model with 50 iterations
	    Predicted Sequential Labels for Test Data
	    --- 109.89823818206787 seconds ---
	python34 evaluate_model.py ../../labeled\ data_76/ 93_7__100_50_advanced_result.txt
	    Overall Accuracy : 82.04324035766716
	mv resultAnalyze.txt 93_7__100_50_resultAnalyze_advanced.txt
	
    Vocareum:
	Trained Advanced_CRF model with 50 iterations 
	Predicted Sequential Labels for Test Data 
	--- 130.00409531593323 seconds --- 
	Number of correct labels: 12404/15268
	Accuracy: 81.2418129421%
=======================================================================================================================================================================================================================================================================================
7. References:
https://web.stanford.edu/~jurafsky/ws97/manual.august1.html
http://sklearn-crfsuite.readthedocs.io/en/latest/tutorial.html
http://python-crfsuite.readthedocs.io/en/latest/pycrfsuite.html#pycrfsuite.Trainer.select
http://python-crfsuite.readthedocs.io/en/latest/_modules/pycrfsuite/_dumpparser.html#ParsedDump
https://labs.vocareum.com/main/main.php?m=editor&nav=1&asnid=8919&stepid=8920#
http://nbviewer.jupyter.org/github/tpeng/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb
